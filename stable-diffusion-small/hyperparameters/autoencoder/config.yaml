shared_configs:
  image_size: 256
  base_channels: 48 #192
  num_groups: 16
  z_dims: 8
  z_scale_factor: 1 # 0.18215

train:
  model_name: "autoencoder"
  checkpoint_dir: "ckpts/model={model_name}/dataset={data_name}/run={run_name}/date={date}/"
  precision: bf16-mixed
  accumulate_grad_batches: 1
  min_wrap_params: 100_000
  max_epochs: 100
  val_check_interval: 0.25 # 0-1 indicates fraction of an epoch. >1 indicates number of batches/steps
  early_stopping_min_delta: 1e-5
  early_stopping_patience: 10
  metric_to_monitor: "val/rec_loss"
  
model:
  target: models.autoencoder.VAutoEncoder
  params:
    lr: 1e-5
    min_beta: 1e-3
    max_beta: 1e-1
    kl_anneal_epochs: 10
    encoder_config:
      target: modules.autoencoder.encoder.VEncoder
      params:
        base_channels: ${shared_configs.base_channels}
        num_groups: ${shared_configs.num_groups}
        z_dims: ${shared_configs.z_dims}
        z_scale_factor: ${shared_configs.z_scale_factor}
    decoder_config:
      target: modules.autoencoder.decoder.VDecoder
      params:
        base_channels: ${shared_configs.base_channels}
        num_groups: ${shared_configs.num_groups}
        z_dims: ${shared_configs.z_dims}
        z_scale_factor: ${shared_configs.z_scale_factor}

    loss_config:
      target: modules.losses.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 5000
        kl_weight: 1e-5
        disc_weight: 0.5

data:
  target: modules.data.autoencoder.AutoEncoderDataModule
  params:
    data_path: "Ryan-sjtu/celebahq-caption" 
    seed: 52    
    batch_size: 16                
    image_size: ${shared_configs.image_size}
    num_workers: 10                   
    buffer_size: 512                
    train_val_split: 0.1                       
    preprocess_batch_fn: "preprocess_celebahq_caption"
    collate_fn: "collate_celebahq_caption"